{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run this notebook directly on Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DaniAffCH/Vessel-Geometric-Transformers/blob/main/main.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "[https://github.com/psf/black] already up to date!\n",
      "[https://github.com/pycqa/isort] already up to date!\n",
      "[https://github.com/PyCQA/flake8] already up to date!\n",
      "[https://github.com/pre-commit/mirrors-mypy] already up to date!\n",
      "pre-commit installed at .git/hooks/pre-commit\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "COLAB_RUNTIME = 'google.colab' in sys.modules\n",
    "if COLAB_RUNTIME:\n",
    "    !git clone https://github.com/DaniAffCH/Vessel-Geometric-Transformers.git\n",
    "    !mv Vessel-Geometric-Transformers/* . \n",
    "    !pip install -q -r requirements.txt\n",
    "else:\n",
    "    !pip install -q -r requirements.txt\n",
    "    !pre-commit autoupdate\n",
    "    !pre-commit install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import load_config\n",
    "import os\n",
    "from config import DatasetConfig, TrainerConfig, BaselineConfig, GatrConfig\n",
    "\n",
    "config_path = os.path.join(\"config\",\"config.yaml\")\n",
    "\n",
    "config = load_config(config_path)\n",
    "dataset_config: DatasetConfig = config.dataset\n",
    "trainer_config: TrainerConfig = config.trainer\n",
    "baseline_config: BaselineConfig = config.baseline\n",
    "gatr_config: GatrConfig = config.gatr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/torch_geometric/data/dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/src/data/dataset.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2999\n",
      "Validation size: 599\n",
      "Test size: 401\n",
      "Data(pos=[8397, 3], wss=[8397, 3], pressure=[8397], face=[3, 16790], inlet_index=[214], label=Category.Single)\n"
     ]
    }
   ],
   "source": [
    "from src.data import VesselDataModule\n",
    "\n",
    "\n",
    "data = VesselDataModule(dataset_config)\n",
    "print(f'Train size: {len(data.train_set)}')\n",
    "print(f'Validation size: {len(data.val_set)}')\n",
    "print(f'Test size: {len(data.test_set)}')\n",
    "print(data.train_set[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: analisi dati preliminare. Classi bilanciate? Statistiche sul dataset? Qualche plot figo.\n",
    "\n",
    "# TODO: equivariance check! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:daco8vob) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>test/acc</td><td>▁▁</td></tr><tr><td>test/f1</td><td>▁▁</td></tr><tr><td>test/loss</td><td>▁▁</td></tr><tr><td>train/acc</td><td>▁▆▇█████████</td></tr><tr><td>train/f1</td><td>▁▆▇█████████</td></tr><tr><td>train/loss</td><td>█▃▂▁▁▁▂▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▄▄▅▅▅▅▆▆▇▇▇▇████</td></tr><tr><td>val/acc</td><td>▂▁▇▇▇▇▇▇████</td></tr><tr><td>val/f1</td><td>▂▁▇▇▇▇▇▇████</td></tr><tr><td>val/loss</td><td>▆█▂▂▁▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>test/acc</td><td>1.0</td></tr><tr><td>test/f1</td><td>1.0</td></tr><tr><td>test/loss</td><td>1e-05</td></tr><tr><td>train/acc</td><td>1.0</td></tr><tr><td>train/f1</td><td>1.0</td></tr><tr><td>train/loss</td><td>1e-05</td></tr><tr><td>trainer/global_step</td><td>9000</td></tr><tr><td>val/acc</td><td>1.0</td></tr><tr><td>val/f1</td><td>1.0</td></tr><tr><td>val/loss</td><td>6e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">toasty-oath-110</strong> at: <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/daco8vob' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/daco8vob</a><br/> View project at: <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240905_105224-daco8vob/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:daco8vob). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/wandb/run-20240905_105743-snddgpk9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/snddgpk9' target=\"_blank\">floral-frog-111</a></strong> to <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/snddgpk9' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/snddgpk9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type              | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | hsProjection    | EquiLinearLayer   | 576    | train\n",
      "1 | backbone        | ModuleList        | 25.3 K | train\n",
      "2 | finalProjection | Linear            | 76.8 K | train\n",
      "3 | loss_fn         | BCEWithLogitsLoss | 0      | train\n",
      "4 | train_accuracy  | BinaryAccuracy    | 0      | train\n",
      "5 | val_accuracy    | BinaryAccuracy    | 0      | train\n",
      "6 | test_accuracy   | BinaryAccuracy    | 0      | train\n",
      "7 | train_f1        | BinaryF1Score     | 0      | train\n",
      "8 | val_f1          | BinaryF1Score     | 0      | train\n",
      "9 | test_f1         | BinaryF1Score     | 0      | train\n",
      "--------------------------------------------------------------\n",
      "102 K     Trainable params\n",
      "0         Non-trainable params\n",
      "102 K     Total params\n",
      "0.411     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 750/750 [00:24<00:00, 30.33it/s, v_num=gpk9, val/loss=0.0296, val/acc=0.990, val/f1=0.990, train/loss=0.163, train/acc=0.964, train/f1=0.964]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 0.030\n",
      "Epoch 0, global step 750: 'val/loss' reached 0.02961 (best 0.02961), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=0-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 750/750 [00:22<00:00, 33.13it/s, v_num=gpk9, val/loss=0.0146, val/acc=0.995, val/f1=0.995, train/loss=0.0304, train/acc=0.990, train/f1=0.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.015 >= min_delta = 1e-05. New best score: 0.015\n",
      "Epoch 1, global step 1500: 'val/loss' reached 0.01461 (best 0.01461), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=1-step=1500.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [00:24<00:00, 30.82it/s, v_num=gpk9, val/loss=0.0141, val/acc=0.993, val/f1=0.993, train/loss=0.0167, train/acc=0.995, train/f1=0.995]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 1e-05. New best score: 0.014\n",
      "Epoch 2, global step 2250: 'val/loss' reached 0.01415 (best 0.01415), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=2-step=2250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 750/750 [00:22<00:00, 34.00it/s, v_num=gpk9, val/loss=0.00846, val/acc=0.997, val/f1=0.997, train/loss=0.00894, train/acc=0.997, train/f1=0.997]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.006 >= min_delta = 1e-05. New best score: 0.008\n",
      "Epoch 3, global step 3000: 'val/loss' reached 0.00846 (best 0.00846), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=3-step=3000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:21<00:00, 35.70it/s, v_num=gpk9, val/loss=0.00364, val/acc=0.998, val/f1=0.998, train/loss=0.00565, train/acc=0.997, train/f1=0.997]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.005 >= min_delta = 1e-05. New best score: 0.004\n",
      "Epoch 4, global step 3750: 'val/loss' reached 0.00364 (best 0.00364), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=4-step=3750-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 750/750 [00:22<00:00, 33.02it/s, v_num=gpk9, val/loss=0.00333, val/acc=0.998, val/f1=0.998, train/loss=0.00599, train/acc=0.998, train/f1=0.998]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 1e-05. New best score: 0.003\n",
      "Epoch 5, global step 4500: 'val/loss' reached 0.00333 (best 0.00333), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=5-step=4500-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 750/750 [00:25<00:00, 29.17it/s, v_num=gpk9, val/loss=0.0118, val/acc=0.997, val/f1=0.997, train/loss=0.0034, train/acc=0.999, train/f1=0.999]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 5250: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 750/750 [00:21<00:00, 35.28it/s, v_num=gpk9, val/loss=0.000665, val/acc=1.000, val/f1=1.000, train/loss=0.000889, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.003 >= min_delta = 1e-05. New best score: 0.001\n",
      "Epoch 7, global step 6000: 'val/loss' reached 0.00066 (best 0.00066), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=7-step=6000.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 750/750 [00:23<00:00, 32.05it/s, v_num=gpk9, val/loss=0.00131, val/acc=1.000, val/f1=1.000, train/loss=0.00562, train/acc=0.998, train/f1=0.998]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 6750: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 750/750 [00:23<00:00, 32.17it/s, v_num=gpk9, val/loss=0.00258, val/acc=0.998, val/f1=0.998, train/loss=0.000814, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 7500: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 750/750 [00:23<00:00, 32.06it/s, v_num=gpk9, val/loss=0.000253, val/acc=1.000, val/f1=1.000, train/loss=0.000704, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 1e-05. New best score: 0.000\n",
      "Epoch 10, global step 8250: 'val/loss' reached 0.00025 (best 0.00025), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=10-step=8250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 750/750 [00:24<00:00, 30.37it/s, v_num=gpk9, val/loss=0.000168, val/acc=1.000, val/f1=1.000, train/loss=0.000194, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 1e-05. New best score: 0.000\n",
      "Epoch 11, global step 9000: 'val/loss' reached 0.00017 (best 0.00017), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=11-step=9000-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 750/750 [00:26<00:00, 28.27it/s, v_num=gpk9, val/loss=0.000756, val/acc=1.000, val/f1=1.000, train/loss=8.27e-5, train/acc=1.000, train/f1=1.000] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 9750: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 750/750 [00:26<00:00, 28.66it/s, v_num=gpk9, val/loss=0.000644, val/acc=1.000, val/f1=1.000, train/loss=0.00276, train/acc=0.999, train/f1=0.999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 10500: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 750/750 [00:25<00:00, 29.17it/s, v_num=gpk9, val/loss=0.00746, val/acc=0.997, val/f1=0.997, train/loss=0.000274, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val/loss did not improve in the last 3 records. Best score: 0.000. Signaling Trainer to stop.\n",
      "Epoch 14, global step 11250: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 750/750 [00:25<00:00, 29.17it/s, v_num=gpk9, val/loss=0.00746, val/acc=0.997, val/f1=0.997, train/loss=0.000274, train/acc=1.000, train/f1=1.000]\n"
     ]
    }
   ],
   "source": [
    "from src.trainer import VesselTrainer\n",
    "from src.models import Gatr\n",
    "\n",
    "trainer = VesselTrainer(trainer_config)\n",
    "\n",
    "\n",
    "model = Gatr(gatr_config)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=11-step=9000-v1.ckpt\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=11-step=9000-v1.ckpt\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 101/101 [00:02<00:00, 47.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9975062608718872     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.997481107711792     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.007669165730476379    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9975062608718872    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.997481107711792    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.007669165730476379   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:in95pf7f) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test/acc</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/acc</td><td>▁▆▇▇███████████</td></tr><tr><td>train/f1</td><td>▁▆▇▇███████████</td></tr><tr><td>train/loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>val/acc</td><td>▁▆▆▇▆▆▇▇█████▇█</td></tr><tr><td>val/f1</td><td>▁▆▆▇▆▆▇▇█████▇█</td></tr><tr><td>val/loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>test/acc</td><td>0.99751</td></tr><tr><td>test/f1</td><td>0.99748</td></tr><tr><td>test/loss</td><td>0.01285</td></tr><tr><td>train/acc</td><td>1.0</td></tr><tr><td>train/f1</td><td>1.0</td></tr><tr><td>train/loss</td><td>0.00027</td></tr><tr><td>trainer/global_step</td><td>11250</td></tr><tr><td>val/acc</td><td>1.0</td></tr><tr><td>val/f1</td><td>1.0</td></tr><tr><td>val/loss</td><td>0.00103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silver-energy-109</strong> at: <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/in95pf7f' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/in95pf7f</a><br/> View project at: <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240905_104613-in95pf7f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:in95pf7f). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/wandb/run-20240905_105224-daco8vob</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/daco8vob' target=\"_blank\">toasty-oath-110</a></strong> to <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/daco8vob' target=\"_blank\">https://wandb.ai/danieleaffinita2000/Vessel-Geometric-Transformers/runs/daco8vob</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:652: Checkpoint directory /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | encoder        | TransformerEncoder | 2.2 K  | train\n",
      "1 | embedder       | Linear             | 272    | train\n",
      "2 | projection     | Linear             | 19.2 K | train\n",
      "3 | loss_fn        | BCEWithLogitsLoss  | 0      | train\n",
      "4 | train_accuracy | BinaryAccuracy     | 0      | train\n",
      "5 | val_accuracy   | BinaryAccuracy     | 0      | train\n",
      "6 | test_accuracy  | BinaryAccuracy     | 0      | train\n",
      "7 | train_f1       | BinaryF1Score      | 0      | train\n",
      "8 | val_f1         | BinaryF1Score      | 0      | train\n",
      "9 | test_f1        | BinaryF1Score      | 0      | train\n",
      "--------------------------------------------------------------\n",
      "21.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.7 K    Total params\n",
      "0.087     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 750/750 [00:22<00:00, 32.63it/s, v_num=8vob, val/loss=0.049, val/acc=0.985, val/f1=0.984, train/loss=0.224, train/acc=0.912, train/f1=0.913]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 0.049\n",
      "Epoch 0, global step 750: 'val/loss' reached 0.04897 (best 0.04897), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=0-step=750.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 750/750 [00:24<00:00, 30.17it/s, v_num=8vob, val/loss=0.0742, val/acc=0.982, val/f1=0.981, train/loss=0.0651, train/acc=0.977, train/f1=0.978]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 1500: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 750/750 [00:25<00:00, 29.68it/s, v_num=8vob, val/loss=0.0086, val/acc=0.998, val/f1=0.998, train/loss=0.0324, train/acc=0.991, train/f1=0.991]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.040 >= min_delta = 1e-05. New best score: 0.009\n",
      "Epoch 2, global step 2250: 'val/loss' reached 0.00860 (best 0.00860), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=2-step=2250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 750/750 [00:23<00:00, 31.77it/s, v_num=8vob, val/loss=0.011, val/acc=0.998, val/f1=0.998, train/loss=0.00862, train/acc=0.997, train/f1=0.997]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 3000: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 750/750 [00:22<00:00, 33.75it/s, v_num=8vob, val/loss=0.00509, val/acc=0.998, val/f1=0.998, train/loss=0.00452, train/acc=0.999, train/f1=0.999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.004 >= min_delta = 1e-05. New best score: 0.005\n",
      "Epoch 4, global step 3750: 'val/loss' reached 0.00509 (best 0.00509), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=4-step=3750-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 750/750 [00:21<00:00, 34.38it/s, v_num=8vob, val/loss=0.00487, val/acc=0.998, val/f1=0.998, train/loss=0.00203, train/acc=0.999, train/f1=0.999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.000 >= min_delta = 1e-05. New best score: 0.005\n",
      "Epoch 5, global step 4500: 'val/loss' reached 0.00487 (best 0.00487), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=5-step=4500-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 750/750 [00:21<00:00, 35.24it/s, v_num=8vob, val/loss=0.00325, val/acc=0.998, val/f1=0.998, train/loss=0.0209, train/acc=0.996, train/f1=0.996] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.002 >= min_delta = 1e-05. New best score: 0.003\n",
      "Epoch 6, global step 5250: 'val/loss' reached 0.00325 (best 0.00325), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=6-step=5250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 750/750 [00:21<00:00, 35.47it/s, v_num=8vob, val/loss=0.0142, val/acc=0.997, val/f1=0.997, train/loss=0.0168, train/acc=0.997, train/f1=0.997] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 6000: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 750/750 [00:24<00:00, 30.71it/s, v_num=8vob, val/loss=2.63e-5, val/acc=1.000, val/f1=1.000, train/loss=0.00471, train/acc=0.999, train/f1=0.999]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.003 >= min_delta = 1e-05. New best score: 0.000\n",
      "Epoch 8, global step 6750: 'val/loss' reached 0.00003 (best 0.00003), saving model to '/home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=8-step=6750-v1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 750/750 [00:24<00:00, 31.13it/s, v_num=8vob, val/loss=3.87e-5, val/acc=1.000, val/f1=1.000, train/loss=1.04e-5, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 7500: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 750/750 [00:23<00:00, 31.52it/s, v_num=8vob, val/loss=5.18e-5, val/acc=1.000, val/f1=1.000, train/loss=8.12e-6, train/acc=1.000, train/f1=1.000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 8250: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 750/750 [00:24<00:00, 30.49it/s, v_num=8vob, val/loss=6e-5, val/acc=1.000, val/f1=1.000, train/loss=7.52e-6, train/acc=1.000, train/f1=1.000]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val/loss did not improve in the last 3 records. Best score: 0.000. Signaling Trainer to stop.\n",
      "Epoch 11, global step 9000: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 750/750 [00:24<00:00, 30.49it/s, v_num=8vob, val/loss=6e-5, val/acc=1.000, val/f1=1.000, train/loss=7.52e-6, train/acc=1.000, train/f1=1.000]\n"
     ]
    }
   ],
   "source": [
    "from src.trainer import VesselTrainer\n",
    "from src.models import BaselineTransformer\n",
    "\n",
    "trainer = VesselTrainer(trainer_config)\n",
    "\n",
    "model = BaselineTransformer(baseline_config)\n",
    "\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=8-step=6750-v1.ckpt\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/daniaffch/Uni/Deep_Learning/Vessel-Geometric-Transformers/ckpt/epoch=8-step=6750-v1.ckpt\n",
      "/home/daniaffch/Desktop/Uni/Deep_Learning/Vessel-Geometric-Transformers/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 101/101 [00:02<00:00, 41.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          test/f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            1.0            </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test/loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   1.246781994268531e-05   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m         test/f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           1.0           \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test/loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  1.246781994268531e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.test(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
